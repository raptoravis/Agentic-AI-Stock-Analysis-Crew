2025-02-04 21:46:49,067 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-02-04 21:46:49,068 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: AAPL
2025-02-04 21:46:49,068 - market_analysis_crew - INFO - Creating specialized agents
2025-02-04 21:46:49,079 - market_analysis_crew - INFO - Creating analysis tasks for ticker: AAPL
2025-02-04 21:46:49,080 - market_analysis_crew - ERROR - Error during stock analysis: 1 validation error for Task
expected_output
  Field required [type=missing, input_value={'description': 'Conduct ...ehensive market views.)}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.10/v/missing
2025-02-04 21:48:13,387 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-02-04 21:48:13,388 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: AAPL
2025-02-04 21:48:13,388 - market_analysis_crew - INFO - Creating specialized agents
2025-02-04 21:48:13,399 - market_analysis_crew - INFO - Creating analysis tasks for ticker: AAPL
2025-02-04 21:48:13,402 - market_analysis_crew - ERROR - Error during stock analysis: 1 validation error for Crew
verbose
  Input should be a valid boolean, unable to interpret input [type=bool_parsing, input_value=2, input_type=int]
    For further information visit https://errors.pydantic.dev/2.10/v/bool_parsing
2025-02-04 21:49:01,898 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-02-04 21:49:01,898 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: AAPL
2025-02-04 21:49:01,899 - market_analysis_crew - INFO - Creating specialized agents
2025-02-04 21:49:01,906 - market_analysis_crew - INFO - Creating analysis tasks for ticker: AAPL
2025-02-04 21:49:01,940 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 21:49:02,078 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=your_key "HTTP/1.1 400 Bad Request"
2025-02-04 21:49:02,092 - root - ERROR - LiteLLM call failed: litellm.AuthenticationError: geminiException - {
  "error": {
    "code": 400,
    "message": "API key not valid. Please pass a valid API key.",
    "status": "INVALID_ARGUMENT",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.ErrorInfo",
        "reason": "API_KEY_INVALID",
        "domain": "googleapis.com",
        "metadata": {
          "service": "generativelanguage.googleapis.com"
        }
      },
      {
        "@type": "type.googleapis.com/google.rpc.LocalizedMessage",
        "locale": "en-US",
        "message": "API key not valid. Please pass a valid API key."
      }
    ]
  }
}

2025-02-04 21:49:02,092 - market_analysis_crew - ERROR - Error during stock analysis: litellm.AuthenticationError: geminiException - {
  "error": {
    "code": 400,
    "message": "API key not valid. Please pass a valid API key.",
    "status": "INVALID_ARGUMENT",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.ErrorInfo",
        "reason": "API_KEY_INVALID",
        "domain": "googleapis.com",
        "metadata": {
          "service": "generativelanguage.googleapis.com"
        }
      },
      {
        "@type": "type.googleapis.com/google.rpc.LocalizedMessage",
        "locale": "en-US",
        "message": "API key not valid. Please pass a valid API key."
      }
    ]
  }
}

2025-02-04 21:50:14,414 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-02-04 21:50:14,414 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: AAPL
2025-02-04 21:50:14,415 - market_analysis_crew - INFO - Creating specialized agents
2025-02-04 21:50:14,424 - market_analysis_crew - INFO - Creating analysis tasks for ticker: AAPL
2025-02-04 21:50:14,428 - opentelemetry.trace - WARNING - Overriding of current TracerProvider is not allowed
2025-02-04 21:50:14,441 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 21:50:14,545 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=your_key "HTTP/1.1 400 Bad Request"
2025-02-04 21:50:14,553 - root - ERROR - LiteLLM call failed: litellm.AuthenticationError: geminiException - {
  "error": {
    "code": 400,
    "message": "API key not valid. Please pass a valid API key.",
    "status": "INVALID_ARGUMENT",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.ErrorInfo",
        "reason": "API_KEY_INVALID",
        "domain": "googleapis.com",
        "metadata": {
          "service": "generativelanguage.googleapis.com"
        }
      },
      {
        "@type": "type.googleapis.com/google.rpc.LocalizedMessage",
        "locale": "en-US",
        "message": "API key not valid. Please pass a valid API key."
      }
    ]
  }
}

2025-02-04 21:50:14,553 - market_analysis_crew - ERROR - Error during stock analysis: litellm.AuthenticationError: geminiException - {
  "error": {
    "code": 400,
    "message": "API key not valid. Please pass a valid API key.",
    "status": "INVALID_ARGUMENT",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.ErrorInfo",
        "reason": "API_KEY_INVALID",
        "domain": "googleapis.com",
        "metadata": {
          "service": "generativelanguage.googleapis.com"
        }
      },
      {
        "@type": "type.googleapis.com/google.rpc.LocalizedMessage",
        "locale": "en-US",
        "message": "API key not valid. Please pass a valid API key."
      }
    ]
  }
}

2025-02-04 21:50:47,023 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-02-04 21:50:47,023 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: AAPL
2025-02-04 21:50:47,024 - market_analysis_crew - INFO - Creating specialized agents
2025-02-04 21:50:47,030 - market_analysis_crew - INFO - Creating analysis tasks for ticker: AAPL
2025-02-04 21:50:47,036 - opentelemetry.trace - WARNING - Overriding of current TracerProvider is not allowed
2025-02-04 21:50:47,047 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 21:50:47,151 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=your_key "HTTP/1.1 400 Bad Request"
2025-02-04 21:50:47,158 - root - ERROR - LiteLLM call failed: litellm.AuthenticationError: geminiException - {
  "error": {
    "code": 400,
    "message": "API key not valid. Please pass a valid API key.",
    "status": "INVALID_ARGUMENT",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.ErrorInfo",
        "reason": "API_KEY_INVALID",
        "domain": "googleapis.com",
        "metadata": {
          "service": "generativelanguage.googleapis.com"
        }
      },
      {
        "@type": "type.googleapis.com/google.rpc.LocalizedMessage",
        "locale": "en-US",
        "message": "API key not valid. Please pass a valid API key."
      }
    ]
  }
}

2025-02-04 21:50:47,158 - market_analysis_crew - ERROR - Error during stock analysis: litellm.AuthenticationError: geminiException - {
  "error": {
    "code": 400,
    "message": "API key not valid. Please pass a valid API key.",
    "status": "INVALID_ARGUMENT",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.ErrorInfo",
        "reason": "API_KEY_INVALID",
        "domain": "googleapis.com",
        "metadata": {
          "service": "generativelanguage.googleapis.com"
        }
      },
      {
        "@type": "type.googleapis.com/google.rpc.LocalizedMessage",
        "locale": "en-US",
        "message": "API key not valid. Please pass a valid API key."
      }
    ]
  }
}

2025-02-04 21:51:04,837 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-02-04 21:51:04,838 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: AAPL
2025-02-04 21:51:04,839 - market_analysis_crew - INFO - Creating specialized agents
2025-02-04 21:51:04,851 - market_analysis_crew - INFO - Creating analysis tasks for ticker: AAPL
2025-02-04 21:51:04,873 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 21:51:27,722 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 21:51:27,724 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 21:51:28,643 - primp - INFO - response: https://html.duckduckgo.com/html 200 26548
2025-02-04 21:51:28,649 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 21:51:47,611 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 21:51:47,619 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 21:51:47,622 - financial_tools - INFO - Fetching stock data for ticker: AAPL
2025-02-04 21:51:48,635 - financial_tools - INFO - Successfully fetched stock data for AAPL
2025-02-04 21:51:48,639 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 21:52:46,853 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 21:52:46,855 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 21:52:46,875 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 21:53:01,168 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 21:53:01,170 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 21:53:01,177 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 21:53:13,189 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 21:53:13,192 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 21:53:13,211 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 21:53:32,710 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 21:53:32,712 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 21:53:32,723 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 21:54:06,762 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 21:54:06,763 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 21:54:06,766 - financial_tools - INFO - Calculating financial metrics for ticker: AAPL
2025-02-04 21:54:07,475 - financial_tools - INFO - Successfully calculated financial metrics for AAPL
2025-02-04 21:54:07,479 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 21:54:34,236 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 21:54:34,238 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 21:54:34,352 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 21:54:51,873 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 21:54:51,874 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 21:54:51,881 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 21:55:22,917 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 21:55:22,919 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 21:55:22,924 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 21:55:48,124 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 21:55:48,126 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 21:55:48,147 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 21:56:46,776 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 21:56:46,780 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 21:56:46,789 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 21:57:07,774 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 21:57:07,778 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 21:57:07,795 - financial_tools - INFO - Fetching stock data for ticker: AAPL
2025-02-04 21:57:08,289 - financial_tools - INFO - Successfully fetched stock data for AAPL
2025-02-04 21:57:08,306 - market_analysis_crew - INFO - Successfully completed analysis for AAPL
2025-02-04 21:57:08,307 - financial_tools - INFO - Fetching stock data for ticker: AAPL
2025-02-04 21:57:08,398 - financial_tools - INFO - Successfully fetched stock data for AAPL
2025-02-04 21:57:08,399 - financial_tools - INFO - Calculating financial metrics for ticker: AAPL
2025-02-04 21:57:08,485 - financial_tools - INFO - Successfully calculated financial metrics for AAPL
2025-02-04 22:05:26,755 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-02-04 22:05:26,756 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: NVDA
2025-02-04 22:05:26,756 - market_analysis_crew - INFO - Creating specialized agents
2025-02-04 22:05:26,774 - market_analysis_crew - INFO - Creating analysis tasks for ticker: NVDA
2025-02-04 22:05:26,781 - opentelemetry.trace - WARNING - Overriding of current TracerProvider is not allowed
2025-02-04 22:05:26,795 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:05:47,393 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:05:47,395 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:05:48,590 - primp - INFO - response: https://lite.duckduckgo.com/lite/ 200 17710
2025-02-04 22:05:48,594 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:06:05,898 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:06:05,899 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:06:06,962 - primp - INFO - response: https://html.duckduckgo.com/html 200 28129
2025-02-04 22:06:06,966 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:06:27,595 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:06:27,599 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:06:28,492 - primp - INFO - response: https://html.duckduckgo.com/html 200 28176
2025-02-04 22:06:28,497 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:06:52,147 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:06:52,149 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:06:52,899 - primp - INFO - response: https://lite.duckduckgo.com/lite/ 200 18094
2025-02-04 22:06:52,902 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:07:09,112 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:07:09,114 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:07:10,034 - primp - INFO - response: https://lite.duckduckgo.com/lite/ 200 19311
2025-02-04 22:07:10,038 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:07:30,465 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:07:30,466 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:07:31,262 - primp - INFO - response: https://lite.duckduckgo.com/lite/ 200 19553
2025-02-04 22:07:31,266 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:07:40,814 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:07:40,817 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:07:40,835 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:07:54,416 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:07:54,418 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:07:54,420 - financial_tools - INFO - Fetching stock data for ticker: NVDA
2025-02-04 22:07:55,079 - financial_tools - INFO - Successfully fetched stock data for NVDA
2025-02-04 22:07:55,084 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:08:08,335 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:08:08,339 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:08:08,357 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:09:00,240 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:09:00,243 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:09:00,248 - financial_tools - INFO - Calculating financial metrics for ticker: NVDA
2025-02-04 22:09:01,027 - financial_tools - INFO - Successfully calculated financial metrics for NVDA
2025-02-04 22:09:01,030 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:09:20,814 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:09:20,816 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:09:20,824 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:09:42,689 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:09:42,690 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:09:42,709 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:10:00,357 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:10:00,359 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:10:00,366 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:10:33,331 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:10:33,332 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:10:33,338 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:10:51,501 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:10:51,503 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:10:51,512 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:11:06,140 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:11:06,142 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:11:06,150 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:11:25,471 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:11:25,474 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:11:25,494 - financial_tools - INFO - Fetching stock data for ticker: NVDA
2025-02-04 22:11:25,839 - financial_tools - INFO - Successfully fetched stock data for NVDA
2025-02-04 22:11:25,843 - market_analysis_crew - INFO - Successfully completed analysis for NVDA
2025-02-04 22:11:25,843 - financial_tools - INFO - Fetching stock data for ticker: NVDA
2025-02-04 22:11:25,921 - financial_tools - INFO - Successfully fetched stock data for NVDA
2025-02-04 22:11:25,922 - financial_tools - INFO - Calculating financial metrics for ticker: NVDA
2025-02-04 22:11:25,997 - financial_tools - INFO - Successfully calculated financial metrics for NVDA
2025-02-04 22:23:09,590 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-02-04 22:23:09,591 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: AAPL
2025-02-04 22:23:09,591 - market_analysis_crew - INFO - Creating specialized agents
2025-02-04 22:23:09,595 - market_analysis_crew - ERROR - Error during stock analysis: "EnhancedAgent" object has no field "callback"
2025-02-04 22:23:45,883 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-02-04 22:23:45,884 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: AAPL
2025-02-04 22:23:45,884 - market_analysis_crew - INFO - Creating specialized agents
2025-02-04 22:23:45,885 - market_analysis_crew - ERROR - Error during stock analysis: "EnhancedAgent" object has no field "callback"
2025-02-04 22:24:33,880 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-02-04 22:24:33,881 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: AAPL
2025-02-04 22:24:33,881 - market_analysis_crew - INFO - Creating specialized agents
2025-02-04 22:24:33,892 - market_analysis_crew - INFO - Creating analysis tasks for ticker: AAPL
2025-02-04 22:24:33,893 - market_analysis_crew - ERROR - Error during stock analysis: 'EnhancedAgent' object has no attribute 'get'
2025-02-04 22:25:21,516 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-02-04 22:25:21,517 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: AAPL
2025-02-04 22:25:21,519 - market_analysis_crew - INFO - Creating specialized agents
2025-02-04 22:25:21,526 - market_analysis_crew - INFO - Creating analysis tasks for ticker: AAPL
2025-02-04 22:25:21,527 - market_analysis_crew - ERROR - Error during stock analysis: 'Agent' object has no attribute 'get'
2025-02-04 22:26:14,503 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-02-04 22:26:14,504 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: AAPL
2025-02-04 22:26:14,504 - market_analysis_crew - INFO - Creating specialized agents
2025-02-04 22:26:14,507 - market_analysis_crew - ERROR - Error during stock analysis: "EnhancedAgent" object has no field "callback"
2025-02-04 22:26:53,212 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-02-04 22:26:53,214 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: AAPL
2025-02-04 22:26:53,214 - market_analysis_crew - INFO - Creating specialized agents
2025-02-04 22:26:53,221 - market_analysis_crew - INFO - Creating analysis tasks for ticker: AAPL
2025-02-04 22:26:53,228 - opentelemetry.trace - WARNING - Overriding of current TracerProvider is not allowed
2025-02-04 22:26:53,241 - market_analysis_crew - ERROR - Error during stock analysis: EnhancedAgent.execute_task() got an unexpected keyword argument 'context'
2025-02-04 22:27:26,363 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-02-04 22:27:26,364 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: AAPL
2025-02-04 22:27:26,364 - market_analysis_crew - INFO - Creating specialized agents
2025-02-04 22:27:26,372 - market_analysis_crew - INFO - Creating analysis tasks for ticker: AAPL
2025-02-04 22:27:26,376 - opentelemetry.trace - WARNING - Overriding of current TracerProvider is not allowed
2025-02-04 22:27:26,387 - market_analysis_crew - ERROR - Error during stock analysis: EnhancedAgent.execute_task() got an unexpected keyword argument 'tools'
2025-02-04 22:28:23,361 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-02-04 22:28:23,362 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: AAPL
2025-02-04 22:28:23,363 - market_analysis_crew - INFO - Creating specialized agents
2025-02-04 22:28:23,375 - market_analysis_crew - INFO - Creating analysis tasks for ticker: AAPL
2025-02-04 22:28:23,380 - opentelemetry.trace - WARNING - Overriding of current TracerProvider is not allowed
2025-02-04 22:28:26,397 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:28:48,381 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:28:48,385 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:28:49,462 - primp - INFO - response: https://html.duckduckgo.com/html 200 26738
2025-02-04 22:28:49,467 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:29:09,003 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:29:09,004 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:29:09,007 - financial_tools - INFO - Fetching stock data for ticker: AAPL
2025-02-04 22:29:09,376 - financial_tools - INFO - Successfully fetched stock data for AAPL
2025-02-04 22:29:09,380 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:30:09,598 - root - ERROR - LiteLLM call failed: litellm.APIConnectionError: GeminiException - Server disconnected without sending a response.
2025-02-04 22:30:09,598 - market_analysis_crew - ERROR - Error during stock analysis: litellm.APIConnectionError: GeminiException - Server disconnected without sending a response.
2025-02-04 22:30:26,171 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-02-04 22:30:26,172 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: AAPL
2025-02-04 22:30:26,172 - market_analysis_crew - INFO - Creating specialized agents
2025-02-04 22:30:26,181 - market_analysis_crew - INFO - Creating analysis tasks for ticker: AAPL
2025-02-04 22:30:26,186 - opentelemetry.trace - WARNING - Overriding of current TracerProvider is not allowed
2025-02-04 22:30:29,215 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:30:47,497 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:30:47,499 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:30:48,433 - primp - INFO - response: https://lite.duckduckgo.com/lite/ 200 18492
2025-02-04 22:30:48,436 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:31:11,087 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:31:11,092 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:31:11,094 - financial_tools - INFO - Fetching stock data for ticker: AAPL
2025-02-04 22:31:11,383 - financial_tools - INFO - Successfully fetched stock data for AAPL
2025-02-04 22:31:11,389 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:31:47,078 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:31:47,080 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:31:47,880 - primp - INFO - response: https://lite.duckduckgo.com/lite/ 200 18192
2025-02-04 22:31:47,884 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:32:31,778 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:32:31,791 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:32:32,748 - primp - INFO - response: https://html.duckduckgo.com/html 200 28153
2025-02-04 22:32:32,763 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:32:59,121 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:32:59,123 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:33:00,091 - primp - INFO - response: https://html.duckduckgo.com/html 200 28419
2025-02-04 22:33:00,096 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:33:28,318 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:33:28,319 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:33:29,139 - primp - INFO - response: https://lite.duckduckgo.com/lite/ 200 20122
2025-02-04 22:33:29,143 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:33:45,993 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:33:45,995 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:33:49,027 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:34:05,300 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:34:05,303 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:34:05,312 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:34:14,303 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:34:14,305 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:34:17,324 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:34:38,291 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:34:38,294 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:34:38,297 - financial_tools - INFO - Calculating financial metrics for ticker: AAPL
2025-02-04 22:34:38,567 - financial_tools - INFO - Successfully calculated financial metrics for AAPL
2025-02-04 22:34:38,570 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:34:53,333 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:34:53,335 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:34:53,343 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:35:16,195 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:35:16,197 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:35:19,216 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:35:36,594 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:35:36,602 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:35:36,610 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:36:01,135 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:36:01,138 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:36:01,143 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:36:18,083 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:36:18,085 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:36:21,104 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:36:35,582 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:36:35,584 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:36:35,591 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:36:53,411 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:36:53,414 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:36:53,432 - financial_tools - INFO - Fetching stock data for ticker: AAPL
2025-02-04 22:36:53,859 - financial_tools - INFO - Successfully fetched stock data for AAPL
2025-02-04 22:36:53,862 - market_analysis_crew - INFO - Successfully completed analysis for AAPL
2025-02-04 22:36:53,863 - financial_tools - INFO - Fetching stock data for ticker: AAPL
2025-02-04 22:36:54,078 - financial_tools - INFO - Successfully fetched stock data for AAPL
2025-02-04 22:36:54,079 - financial_tools - INFO - Calculating financial metrics for ticker: AAPL
2025-02-04 22:36:54,187 - financial_tools - INFO - Successfully calculated financial metrics for AAPL
2025-02-04 22:45:55,484 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-02-04 22:45:55,485 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: META
2025-02-04 22:45:55,485 - market_analysis_crew - INFO - Creating specialized agents
2025-02-04 22:45:55,503 - market_analysis_crew - INFO - Creating analysis tasks for ticker: META
2025-02-04 22:45:55,510 - opentelemetry.trace - WARNING - Overriding of current TracerProvider is not allowed
2025-02-04 22:45:58,527 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:46:21,253 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:46:21,256 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:46:22,171 - primp - INFO - response: https://lite.duckduckgo.com/lite/ 200 18827
2025-02-04 22:46:22,177 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:46:45,486 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:46:45,488 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:46:46,240 - primp - INFO - response: https://lite.duckduckgo.com/lite/ 200 20221
2025-02-04 22:46:46,246 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:47:04,130 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:47:04,132 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:47:05,003 - primp - INFO - response: https://html.duckduckgo.com/html 200 27689
2025-02-04 22:47:05,009 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:47:19,835 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:47:19,837 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:47:20,714 - primp - INFO - response: https://html.duckduckgo.com/html 200 35697
2025-02-04 22:47:20,718 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:47:39,036 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:47:39,038 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:47:39,905 - primp - INFO - response: https://lite.duckduckgo.com/lite/ 200 19042
2025-02-04 22:47:39,911 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:47:58,704 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:47:58,705 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:47:59,509 - primp - INFO - response: https://html.duckduckgo.com/html 200 29274
2025-02-04 22:47:59,517 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:48:12,868 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:48:12,869 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:48:15,898 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:48:31,151 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:48:31,153 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:48:31,158 - financial_tools - INFO - Fetching stock data for ticker: META
2025-02-04 22:48:31,812 - financial_tools - INFO - Successfully fetched stock data for META
2025-02-04 22:48:31,817 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:48:43,914 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:48:43,917 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:48:46,949 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:49:10,352 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:49:10,354 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:49:10,357 - financial_tools - INFO - Calculating financial metrics for ticker: META
2025-02-04 22:49:11,038 - financial_tools - INFO - Successfully calculated financial metrics for META
2025-02-04 22:49:11,042 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:49:23,506 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:49:23,507 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:49:26,526 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:49:42,936 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:49:42,939 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:49:42,947 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:50:43,052 - root - ERROR - LiteLLM call failed: litellm.APIConnectionError: GeminiException - Server disconnected without sending a response.
2025-02-04 22:50:43,052 - market_analysis_crew - ERROR - Error during stock analysis: litellm.APIConnectionError: GeminiException - Server disconnected without sending a response.
2025-02-04 22:50:56,092 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-02-04 22:50:56,093 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: META
2025-02-04 22:50:56,093 - market_analysis_crew - INFO - Creating specialized agents
2025-02-04 22:50:56,104 - market_analysis_crew - INFO - Creating analysis tasks for ticker: META
2025-02-04 22:50:56,108 - opentelemetry.trace - WARNING - Overriding of current TracerProvider is not allowed
2025-02-04 22:50:59,121 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:51:19,672 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:51:19,674 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:51:20,687 - primp - INFO - response: https://html.duckduckgo.com/html 200 42032
2025-02-04 22:51:20,691 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:51:41,623 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:51:41,626 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:51:42,437 - primp - INFO - response: https://lite.duckduckgo.com/lite/ 200 18737
2025-02-04 22:51:42,444 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:52:11,429 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:52:11,431 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:52:12,443 - primp - INFO - response: https://html.duckduckgo.com/html 200 37979
2025-02-04 22:52:12,447 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:52:31,787 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:52:31,791 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:52:32,587 - primp - INFO - response: https://lite.duckduckgo.com/lite/ 200 19447
2025-02-04 22:52:32,591 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:52:41,542 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:52:41,546 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:52:44,569 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:53:01,035 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:53:01,036 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:53:01,039 - financial_tools - INFO - Fetching stock data for ticker: META
2025-02-04 22:53:01,445 - financial_tools - INFO - Successfully fetched stock data for META
2025-02-04 22:53:01,450 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:54:01,678 - root - ERROR - LiteLLM call failed: litellm.APIConnectionError: GeminiException - Server disconnected without sending a response.
2025-02-04 22:54:01,680 - market_analysis_crew - ERROR - Error during stock analysis: litellm.APIConnectionError: GeminiException - Server disconnected without sending a response.
2025-02-04 22:55:14,261 - market_analysis_crew - INFO - Initializing LLM...
2025-02-04 22:55:21,081 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-02-04 22:55:21,081 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: META
2025-02-04 22:55:21,082 - market_analysis_crew - INFO - Creating specialized agents
2025-02-04 22:55:21,092 - market_analysis_crew - INFO - Creating analysis tasks for ticker: META
2025-02-04 22:55:21,099 - opentelemetry.trace - WARNING - Overriding of current TracerProvider is not allowed
2025-02-04 22:55:24,126 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:55:50,219 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:55:50,222 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:55:51,109 - primp - INFO - response: https://html.duckduckgo.com/html 200 28684
2025-02-04 22:55:51,113 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:56:15,960 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:56:15,962 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:56:15,965 - financial_tools - INFO - Fetching stock data for ticker: META
2025-02-04 22:56:16,219 - financial_tools - INFO - Successfully fetched stock data for META
2025-02-04 22:56:16,236 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:56:53,691 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:56:53,694 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:56:54,447 - primp - INFO - response: https://html.duckduckgo.com/html 200 28838
2025-02-04 22:56:54,451 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:57:38,027 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:57:38,030 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:57:38,929 - primp - INFO - response: https://lite.duckduckgo.com/lite/ 200 18578
2025-02-04 22:57:38,935 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:58:39,051 - root - ERROR - LiteLLM call failed: litellm.APIConnectionError: GeminiException - Server disconnected without sending a response.
2025-02-04 22:58:46,064 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:59:05,137 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:59:05,139 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:59:05,964 - primp - INFO - response: https://lite.duckduckgo.com/lite/ 200 18584
2025-02-04 22:59:05,971 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:59:31,260 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:59:31,261 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:59:32,093 - primp - INFO - response: https://lite.duckduckgo.com/lite/ 200 19191
2025-02-04 22:59:32,096 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 22:59:50,178 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 22:59:50,180 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 22:59:50,188 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:00:16,363 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:00:16,365 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:00:19,387 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:00:35,603 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:00:35,605 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:00:35,612 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:00:49,167 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:00:49,169 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:00:52,191 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:01:18,275 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:01:18,279 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:01:18,283 - financial_tools - INFO - Calculating financial metrics for ticker: META
2025-02-04 23:01:18,549 - financial_tools - INFO - Successfully calculated financial metrics for META
2025-02-04 23:01:18,552 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:01:39,731 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:01:39,733 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:01:39,740 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:02:03,162 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:02:03,166 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:02:06,187 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:02:31,082 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:02:31,085 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:02:31,093 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:02:58,809 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:02:58,812 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:02:58,817 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:03:17,839 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:03:17,841 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:03:20,861 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:03:37,034 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:03:37,035 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:03:37,043 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:03:55,189 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:03:55,192 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:03:55,211 - financial_tools - INFO - Fetching stock data for ticker: META
2025-02-04 23:03:55,589 - financial_tools - INFO - Successfully fetched stock data for META
2025-02-04 23:03:55,592 - market_analysis_crew - INFO - Successfully completed analysis for META
2025-02-04 23:03:55,593 - financial_tools - INFO - Fetching stock data for ticker: META
2025-02-04 23:03:55,670 - financial_tools - INFO - Successfully fetched stock data for META
2025-02-04 23:03:55,671 - financial_tools - INFO - Calculating financial metrics for ticker: META
2025-02-04 23:03:55,764 - financial_tools - INFO - Successfully calculated financial metrics for META
2025-02-04 23:06:40,221 - market_analysis_crew - INFO - Initializing LLM...
2025-02-04 23:07:13,334 - market_analysis_crew - INFO - Initializing LLM...
2025-02-04 23:10:33,721 - market_analysis_crew - INFO - Initializing LLM...
2025-02-04 23:16:15,234 - market_analysis_crew - INFO - Initializing LLM...
2025-02-04 23:16:30,139 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-02-04 23:16:30,140 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: AAPL
2025-02-04 23:16:30,140 - market_analysis_crew - INFO - Creating specialized agents
2025-02-04 23:16:30,148 - market_analysis_crew - INFO - Creating analysis tasks for ticker: AAPL
2025-02-04 23:16:33,188 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:16:56,236 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:16:56,240 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:16:57,164 - primp - INFO - response: https://lite.duckduckgo.com/lite/ 200 18340
2025-02-04 23:16:57,170 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:17:14,936 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:17:14,947 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:17:14,950 - financial_tools - INFO - Fetching stock data for ticker: AAPL
2025-02-04 23:17:15,657 - financial_tools - INFO - Successfully fetched stock data for AAPL
2025-02-04 23:17:15,662 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:17:38,277 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:17:38,279 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:17:39,297 - primp - INFO - response: https://html.duckduckgo.com/html 200 29000
2025-02-04 23:17:39,301 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:18:10,463 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:18:10,466 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:18:11,508 - primp - INFO - response: https://lite.duckduckgo.com/lite/ 200 19675
2025-02-04 23:18:11,514 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:18:30,172 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:18:30,174 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:18:31,079 - primp - INFO - response: https://lite.duckduckgo.com/lite/ 200 18846
2025-02-04 23:18:31,084 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:18:48,043 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:18:48,045 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:18:51,077 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:19:07,062 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:19:07,064 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:19:07,076 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:20:07,268 - root - ERROR - LiteLLM call failed: litellm.APIConnectionError: GeminiException - Server disconnected without sending a response.
2025-02-04 23:20:14,272 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:20:27,135 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:20:27,137 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:20:27,145 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:21:03,125 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:21:03,128 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:21:06,152 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:21:27,405 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:21:27,407 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:21:27,411 - financial_tools - INFO - Calculating financial metrics for ticker: AAPL
2025-02-04 23:21:28,278 - financial_tools - INFO - Successfully calculated financial metrics for AAPL
2025-02-04 23:21:28,282 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:21:41,515 - root - ERROR - LiteLLM call failed: litellm.APIConnectionError: GeminiException - Server disconnected without sending a response.
2025-02-04 23:37:34,606 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:37:48,202 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-02-04 23:37:48,203 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: AAPL
2025-02-04 23:37:48,204 - market_analysis_crew - INFO - Creating specialized agents
2025-02-04 23:37:48,213 - market_analysis_crew - INFO - Creating analysis tasks for ticker: AAPL
2025-02-04 23:37:48,216 - opentelemetry.trace - WARNING - Overriding of current TracerProvider is not allowed
2025-02-04 23:37:51,240 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:38:00,594 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:38:00,596 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:38:00,600 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:38:10,508 - opentelemetry.sdk.trace.export - ERROR - Exception while exporting Span batch.
Traceback (most recent call last):
  File "/home/vscode/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/vscode/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 534, in _make_request
    response = conn.getresponse()
               ^^^^^^^^^^^^^^^^^^
  File "/home/vscode/.local/lib/python3.12/site-packages/urllib3/connection.py", line 516, in getresponse
    httplib_response = super().getresponse()
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/http/client.py", line 1428, in getresponse
    response.begin()
  File "/usr/local/lib/python3.12/http/client.py", line 331, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/http/client.py", line 300, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/vscode/.local/lib/python3.12/site-packages/requests/adapters.py", line 667, in send
    resp = conn.urlopen(
           ^^^^^^^^^^^^^
  File "/home/vscode/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 841, in urlopen
    retries = retries.increment(
              ^^^^^^^^^^^^^^^^^^
  File "/home/vscode/.local/lib/python3.12/site-packages/urllib3/util/retry.py", line 474, in increment
    raise reraise(type(error), error, _stacktrace)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vscode/.local/lib/python3.12/site-packages/urllib3/util/util.py", line 38, in reraise
    raise value.with_traceback(tb)
  File "/home/vscode/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 787, in urlopen
    response = self._make_request(
               ^^^^^^^^^^^^^^^^^^^
  File "/home/vscode/.local/lib/python3.12/site-packages/urllib3/connectionpool.py", line 534, in _make_request
    response = conn.getresponse()
               ^^^^^^^^^^^^^^^^^^
  File "/home/vscode/.local/lib/python3.12/site-packages/urllib3/connection.py", line 516, in getresponse
    httplib_response = super().getresponse()
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/http/client.py", line 1428, in getresponse
    response.begin()
  File "/usr/local/lib/python3.12/http/client.py", line 331, in begin
    version, status, reason = self._read_status()
                              ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/http/client.py", line 300, in _read_status
    raise RemoteDisconnected("Remote end closed connection without"
urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/vscode/.local/lib/python3.12/site-packages/opentelemetry/sdk/trace/export/__init__.py", line 360, in _export_batch
    self.span_exporter.export(self.spans_list[:idx])  # type: ignore
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vscode/.local/lib/python3.12/site-packages/opentelemetry/exporter/otlp/proto/http/trace_exporter/__init__.py", line 189, in export
    return self._export_serialized_spans(serialized_data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vscode/.local/lib/python3.12/site-packages/opentelemetry/exporter/otlp/proto/http/trace_exporter/__init__.py", line 159, in _export_serialized_spans
    resp = self._export(serialized_data)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vscode/.local/lib/python3.12/site-packages/opentelemetry/exporter/otlp/proto/http/trace_exporter/__init__.py", line 133, in _export
    return self._session.post(
           ^^^^^^^^^^^^^^^^^^^
  File "/home/vscode/.local/lib/python3.12/site-packages/requests/sessions.py", line 637, in post
    return self.request("POST", url, data=data, json=json, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vscode/.local/lib/python3.12/site-packages/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vscode/.local/lib/python3.12/site-packages/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vscode/.local/lib/python3.12/site-packages/requests/adapters.py", line 682, in send
    raise ConnectionError(err, request=request)
requests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))
2025-02-04 23:38:14,215 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:38:14,221 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:38:15,075 - primp - INFO - response: https://html.duckduckgo.com/html 200 26929
2025-02-04 23:38:15,081 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:38:20,406 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:38:20,411 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:38:20,421 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:38:34,147 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:38:34,151 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:38:34,157 - financial_tools - INFO - Fetching stock data for ticker: AAPL
2025-02-04 23:38:34,456 - financial_tools - INFO - Successfully fetched stock data for AAPL
2025-02-04 23:38:34,472 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:38:41,879 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:38:41,883 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:38:44,918 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:39:02,068 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:39:02,070 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:39:02,077 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:39:23,121 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:39:23,122 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:39:23,927 - primp - INFO - response: https://html.duckduckgo.com/html 200 26921
2025-02-04 23:39:23,931 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:39:25,625 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:39:25,627 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:39:25,629 - market_analysis_crew - ERROR - Error during stock analysis: EnhancedAgent.execute_task() takes 2 positional arguments but 4 were given
2025-02-04 23:39:30,210 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:39:30,211 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:39:30,217 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:39:49,206 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:39:49,208 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:39:52,228 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:40:06,444 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:40:06,446 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:40:06,456 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:40:24,429 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:40:24,431 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:40:24,450 - financial_tools - INFO - Fetching stock data for ticker: AAPL
2025-02-04 23:40:24,754 - financial_tools - INFO - Successfully fetched stock data for AAPL
2025-02-04 23:40:24,769 - market_analysis_crew - INFO - Successfully completed analysis for AAPL
2025-02-04 23:40:25,714 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-02-04 23:40:25,714 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: META
2025-02-04 23:40:25,715 - market_analysis_crew - INFO - Creating specialized agents
2025-02-04 23:40:25,724 - market_analysis_crew - INFO - Creating analysis tasks for ticker: META
2025-02-04 23:40:25,729 - opentelemetry.trace - WARNING - Overriding of current TracerProvider is not allowed
2025-02-04 23:40:28,744 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:40:47,809 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:40:47,811 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:40:48,533 - primp - INFO - response: https://html.duckduckgo.com/html 200 28668
2025-02-04 23:40:48,539 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:41:04,620 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:41:04,622 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:41:04,624 - financial_tools - INFO - Fetching stock data for ticker: META
2025-02-04 23:41:05,036 - financial_tools - INFO - Successfully fetched stock data for META
2025-02-04 23:41:05,041 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:41:51,728 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:41:51,730 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:41:52,647 - primp - INFO - response: https://lite.duckduckgo.com/lite/ 200 18823
2025-02-04 23:41:52,652 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:41:54,390 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:41:54,393 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:41:54,396 - market_analysis_crew - ERROR - Error during stock analysis: EnhancedAgent.execute_task() takes 2 positional arguments but 4 were given
2025-02-04 23:42:52,437 - market_analysis_crew - INFO - Initializing LLM...
2025-02-04 23:42:58,941 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-02-04 23:42:58,942 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: META
2025-02-04 23:42:58,943 - market_analysis_crew - INFO - Creating specialized agents
2025-02-04 23:42:58,951 - market_analysis_crew - INFO - Creating analysis tasks for ticker: META
2025-02-04 23:42:58,956 - opentelemetry.trace - WARNING - Overriding of current TracerProvider is not allowed
2025-02-04 23:43:01,977 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:43:29,417 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:43:29,430 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:43:30,236 - primp - INFO - response: https://lite.duckduckgo.com/lite/ 200 18695
2025-02-04 23:43:30,250 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:43:49,803 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:43:49,806 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:43:49,808 - financial_tools - INFO - Fetching stock data for ticker: META
2025-02-04 23:43:50,108 - financial_tools - INFO - Successfully fetched stock data for META
2025-02-04 23:43:50,114 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:44:13,415 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:44:13,417 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:44:14,501 - primp - INFO - response: https://html.duckduckgo.com/html 200 28762
2025-02-04 23:44:14,506 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:44:38,954 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:44:38,958 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:44:41,980 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:44:56,717 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:44:56,719 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:44:56,728 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:45:36,058 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:45:36,060 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:45:39,084 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:46:06,417 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:46:06,418 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:46:06,421 - financial_tools - INFO - Calculating financial metrics for ticker: META
2025-02-04 23:46:07,705 - financial_tools - INFO - Successfully calculated financial metrics for META
2025-02-04 23:46:07,708 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:46:25,866 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:46:25,868 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:46:25,874 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:46:54,441 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:46:54,443 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:46:57,463 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:47:20,301 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:47:20,303 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:47:20,312 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:47:59,472 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:47:59,474 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:47:59,480 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:48:18,762 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:48:18,763 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:48:21,786 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:48:37,213 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:48:37,215 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:48:37,222 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-02-04 23:48:55,901 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyBq1fH-4WFbsYLGKo-rXtWFW8BM54VR4XY "HTTP/1.1 200 OK"
2025-02-04 23:48:55,903 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-02-04 23:48:55,920 - financial_tools - INFO - Fetching stock data for ticker: META
2025-02-04 23:48:56,183 - financial_tools - INFO - Successfully fetched stock data for META
2025-02-04 23:48:56,186 - market_analysis_crew - INFO - Successfully completed analysis for META
2025-02-04 23:48:56,187 - financial_tools - INFO - Fetching stock data for ticker: META
2025-02-04 23:48:56,256 - financial_tools - INFO - Successfully fetched stock data for META
2025-02-04 23:48:56,257 - financial_tools - INFO - Calculating financial metrics for ticker: META
2025-02-04 23:48:56,342 - financial_tools - INFO - Successfully calculated financial metrics for META
2025-08-07 03:48:20,074 - market_analysis_crew - INFO - Initializing LLM...
2025-08-07 03:49:05,736 - market_analysis_crew - INFO - Initializing LLM...
2025-08-07 03:49:30,718 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-08-07 03:49:30,720 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: AAPL
2025-08-07 03:49:30,720 - market_analysis_crew - INFO - Creating specialized agents
2025-08-07 03:49:30,721 - market_analysis_crew - ERROR - Error during stock analysis: 2 validation errors for EnhancedAgent
tools.0
  Input should be a valid dictionary or instance of BaseTool [type=model_type, input_value=Tool(name='Search', descr...pper at 0x7f6baf00e340>), input_type=Tool]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
tools.1
  Input should be a valid dictionary or instance of BaseTool [type=model_type, input_value=Tool(name='StockData', de...pper at 0x7f6b58e10d60>), input_type=Tool]
    For further information visit https://errors.pydantic.dev/2.11/v/model_type
2025-08-07 03:52:55,462 - market_analysis_crew - INFO - Initializing LLM...
2025-08-07 03:52:55,463 - market_analysis_crew - INFO - LLM initialized successfully
2025-08-07 03:53:09,138 - market_analysis_crew - INFO - Initializing LLM...
2025-08-07 03:53:09,139 - market_analysis_crew - INFO - LLM initialized successfully
2025-08-07 03:53:09,144 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-08-07 03:53:20,544 - market_analysis_crew - INFO - Initializing LLM...
2025-08-07 03:53:20,545 - market_analysis_crew - ERROR - Configuration error: Missing API key. Please set GOOGLE_API_KEY or GEMINI_API_KEY environment variable.
2025-08-07 03:53:20,550 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-08-07 03:54:30,464 - market_analysis_crew - INFO - Initializing LLM...
2025-08-07 03:54:30,465 - market_analysis_crew - INFO - LLM initialized successfully
2025-08-07 03:54:32,728 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-08-07 03:54:32,730 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: AAPL
2025-08-07 03:54:32,731 - market_analysis_crew - INFO - Creating specialized agents
2025-08-07 03:54:32,736 - market_analysis_crew - INFO - Creating analysis tasks for ticker: AAPL
2025-08-07 03:54:35,781 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-08-07 03:54:35,993 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=your_gemini_api_key_here "HTTP/1.1 400 Bad Request"
2025-08-07 03:54:36,019 - market_analysis_crew - ERROR - Error during stock analysis: litellm.AuthenticationError: geminiException - {
  "error": {
    "code": 400,
    "message": "API key not valid. Please pass a valid API key.",
    "status": "INVALID_ARGUMENT",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.ErrorInfo",
        "reason": "API_KEY_INVALID",
        "domain": "googleapis.com",
        "metadata": {
          "service": "generativelanguage.googleapis.com"
        }
      },
      {
        "@type": "type.googleapis.com/google.rpc.LocalizedMessage",
        "locale": "en-US",
        "message": "API key not valid. Please pass a valid API key."
      }
    ]
  }
}

2025-08-07 03:57:32,495 - market_analysis_crew - INFO - Initializing LLM...
2025-08-07 03:57:32,496 - market_analysis_crew - INFO - LLM initialized successfully
2025-08-07 03:57:53,774 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-08-07 03:57:53,776 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: AAPL
2025-08-07 03:57:53,777 - market_analysis_crew - INFO - Creating specialized agents
2025-08-07 03:57:53,786 - market_analysis_crew - INFO - Creating analysis tasks for ticker: AAPL
2025-08-07 03:57:56,841 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-pro-latest; provider = gemini
2025-08-07 03:57:57,034 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro-latest:generateContent?key=AIzaSyAuomvVGgX-Av5kDvfdZamqLRqrvzfnTiI "HTTP/1.1 429 Too Many Requests"
2025-08-07 03:57:57,060 - market_analysis_crew - ERROR - Error during stock analysis: litellm.RateLimitError: litellm.RateLimitError: VertexAIException - {
  "error": {
    "code": 429,
    "message": "You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.",
    "status": "RESOURCE_EXHAUSTED",
    "details": [
      {
        "@type": "type.googleapis.com/google.rpc.QuotaFailure",
        "violations": [
          {
            "quotaMetric": "generativelanguage.googleapis.com/generate_content_free_tier_requests",
            "quotaId": "GenerateRequestsPerDayPerProjectPerModel-FreeTier",
            "quotaDimensions": {
              "location": "global",
              "model": "gemini-1.5-pro"
            }
          },
          {
            "quotaMetric": "generativelanguage.googleapis.com/generate_content_free_tier_requests",
            "quotaId": "GenerateRequestsPerMinutePerProjectPerModel-FreeTier",
            "quotaDimensions": {
              "location": "global",
              "model": "gemini-1.5-pro"
            }
          },
          {
            "quotaMetric": "generativelanguage.googleapis.com/generate_content_free_tier_input_token_count",
            "quotaId": "GenerateContentInputTokensPerModelPerMinute-FreeTier",
            "quotaDimensions": {
              "model": "gemini-1.5-pro",
              "location": "global"
            }
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.Help",
        "links": [
          {
            "description": "Learn more about Gemini API quotas",
            "url": "https://ai.google.dev/gemini-api/docs/rate-limits"
          }
        ]
      },
      {
        "@type": "type.googleapis.com/google.rpc.RetryInfo",
        "retryDelay": "2s"
      }
    ]
  }
}

2025-08-07 04:11:00,611 - market_analysis_crew - INFO - Initializing LLM...
2025-08-07 04:11:00,611 - market_analysis_crew - INFO - Initializing GEMINI LLM with model: gemini-1.5-pro-latest
2025-08-07 04:11:00,612 - market_analysis_crew - INFO - LLM initialized successfully
2025-08-07 04:11:13,017 - market_analysis_crew - INFO - Initializing LLM...
2025-08-07 04:11:13,018 - market_analysis_crew - INFO - Initializing GEMINI LLM with model: gemini-1.5-pro-latest
2025-08-07 04:11:13,018 - market_analysis_crew - INFO - LLM initialized successfully
2025-08-07 04:11:13,024 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-08-07 04:11:13,024 - market_analysis_crew - INFO - Initializing GEMINI LLM with model: gemini-1.5-flash
2025-08-07 04:11:23,209 - market_analysis_crew - INFO - Initializing LLM...
2025-08-07 04:11:23,222 - market_analysis_crew - ERROR - Configuration error: Missing GEMINI API key. Please set GEMINI_API_KEY environment variable. Please set at least one API key in your .env file.
2025-08-07 04:11:23,228 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-08-07 04:12:31,652 - market_analysis_crew - INFO - Initializing LLM...
2025-08-07 04:12:31,653 - market_analysis_crew - INFO - Initializing GEMINI LLM with model: gemini-1.5-pro-latest
2025-08-07 04:12:31,653 - market_analysis_crew - INFO - LLM initialized successfully
2025-08-07 04:13:52,904 - market_analysis_crew - INFO - Initializing LLM...
2025-08-07 04:13:52,905 - market_analysis_crew - INFO - Initializing GEMINI LLM with model: gemini-1.5-pro-latest
2025-08-07 04:13:52,906 - market_analysis_crew - INFO - LLM initialized successfully
2025-08-07 04:14:10,565 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-08-07 04:14:10,565 - market_analysis_crew - INFO - Initializing GEMINI LLM with model: gemini-1.5-flash
2025-08-07 04:14:10,568 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: AAPL
2025-08-07 04:14:10,568 - market_analysis_crew - INFO - Creating specialized agents
2025-08-07 04:14:10,576 - market_analysis_crew - INFO - Creating analysis tasks for ticker: AAPL
2025-08-07 04:14:13,626 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-flash; provider = gemini
2025-08-07 04:14:14,554 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyAuomvVGgX-Av5kDvfdZamqLRqrvzfnTiI "HTTP/1.1 200 OK"
2025-08-07 04:14:14,568 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-08-07 04:14:14,579 - financial_tools - INFO - Fetching stock data for ticker: AAPL
2025-08-07 04:14:16,692 - financial_tools - INFO - Successfully fetched stock data for AAPL
2025-08-07 04:14:16,705 - LiteLLM - INFO - 
LiteLLM completion() model= gemini-1.5-flash; provider = gemini
2025-08-07 04:14:38,721 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=AIzaSyAuomvVGgX-Av5kDvfdZamqLRqrvzfnTiI "HTTP/1.1 503 Service Unavailable"
2025-08-07 04:14:38,745 - market_analysis_crew - ERROR - Error during stock analysis: litellm.InternalServerError: litellm.InternalServerError: VertexAIException - {
  "error": {
    "code": 503,
    "message": "The model is overloaded. Please try again later.",
    "status": "UNAVAILABLE"
  }
}

2025-12-27 17:48:32,959 - market_analysis_crew - INFO - Initializing LLM...
2025-12-27 17:48:34,975 - market_analysis_crew - ERROR - Configuration error: Missing OPENAI API key. Please set OPENAI_API_KEY environment variable. Available providers with API keys: ollama
2025-12-27 17:48:50,191 - market_analysis_crew - INFO - Initializing LLM...
2025-12-27 17:48:52,211 - market_analysis_crew - ERROR - Configuration error: Missing OPENAI API key. Please set OPENAI_API_KEY environment variable. Available providers with API keys: ollama
2025-12-27 17:49:12,141 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-12-27 17:49:14,194 - market_analysis_crew - INFO - Initializing OLLAMA LLM with model: llama3.2
2025-12-27 17:49:14,199 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: AAPL
2025-12-27 17:49:14,199 - market_analysis_crew - INFO - Creating specialized agents
2025-12-27 17:49:14,202 - market_analysis_crew - INFO - Creating analysis tasks for ticker: AAPL
2025-12-27 17:49:17,264 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2; provider = ollama
2025-12-27 17:49:17,733 - LiteLLM - ERROR - Error creating standard logging object - Missing dependency No module named 'fastapi'. Run `pip install 'litellm[proxy]'`
Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 174, in _make_common_sync_call
    response = sync_httpx_client.post(
        url=api_base,
    ...<8 lines>...
        logging_obj=logging_obj,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'http://localhost:11434/api/generate'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\main.py", line 3108, in completion
    response = base_llm_http_handler.completion(
        model=model,
    ...<13 lines>...
        client=client,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 470, in completion
    response = self._make_common_sync_call(
        sync_httpx_client=sync_httpx_client,
    ...<7 lines>...
        logging_obj=logging_obj,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 199, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 2409, in _handle_error
    raise provider_config.get_error_class(
    ...<3 lines>...
    )
litellm.llms.ollama.common_utils.OllamaError: {"error":"model 'llama3.2' not found"}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\utils.py", line 1207, in wrapper
    result = original_function(*args, **kwargs)
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\main.py", line 3452, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2270, in exception_type
    raise APIConnectionError(
    ...<4 lines>...
    )
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - {"error":"model 'llama3.2' not found"}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\proxy_server.py", line 75, in <module>
    import fastapi
ModuleNotFoundError: No module named 'fastapi'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 4370, in get_standard_logging_object_payload
    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(
        metadata=metadata,
    ...<10 lines>...
        response_id=id,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 3921, in get_standard_logging_metadata
    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(
        start_time=start_time,
        response_id=response_id,
        team_alias=clean_metadata.get("user_api_key_team_alias"),
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 4109, in _generate_cold_storage_object_key
    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\spend_tracking\cold_storage_handler.py", line 67, in _get_configured_cold_storage_custom_logger
    from litellm.proxy.proxy_server import general_settings
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\proxy_server.py", line 80, in <module>
    raise ImportError(f"Missing dependency {e}. Run `pip install 'litellm[proxy]'`")
ImportError: Missing dependency No module named 'fastapi'. Run `pip install 'litellm[proxy]'`
2025-12-27 17:49:24,749 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2; provider = ollama
2025-12-27 17:49:24,785 - LiteLLM - ERROR - Error creating standard logging object - Missing dependency No module named 'fastapi'. Run `pip install 'litellm[proxy]'`
Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 174, in _make_common_sync_call
    response = sync_httpx_client.post(
        url=api_base,
    ...<8 lines>...
        logging_obj=logging_obj,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'http://localhost:11434/api/generate'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\main.py", line 3108, in completion
    response = base_llm_http_handler.completion(
        model=model,
    ...<13 lines>...
        client=client,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 470, in completion
    response = self._make_common_sync_call(
        sync_httpx_client=sync_httpx_client,
    ...<7 lines>...
        logging_obj=logging_obj,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 199, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 2409, in _handle_error
    raise provider_config.get_error_class(
    ...<3 lines>...
    )
litellm.llms.ollama.common_utils.OllamaError: {"error":"model 'llama3.2' not found"}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\utils.py", line 1207, in wrapper
    result = original_function(*args, **kwargs)
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\main.py", line 3452, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2270, in exception_type
    raise APIConnectionError(
    ...<4 lines>...
    )
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - {"error":"model 'llama3.2' not found"}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\proxy_server.py", line 75, in <module>
    import fastapi
ModuleNotFoundError: No module named 'fastapi'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 4370, in get_standard_logging_object_payload
    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(
        metadata=metadata,
    ...<10 lines>...
        response_id=id,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 3921, in get_standard_logging_metadata
    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(
        start_time=start_time,
        response_id=response_id,
        team_alias=clean_metadata.get("user_api_key_team_alias"),
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 4109, in _generate_cold_storage_object_key
    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\spend_tracking\cold_storage_handler.py", line 67, in _get_configured_cold_storage_custom_logger
    from litellm.proxy.proxy_server import general_settings
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\proxy_server.py", line 80, in <module>
    raise ImportError(f"Missing dependency {e}. Run `pip install 'litellm[proxy]'`")
ImportError: Missing dependency No module named 'fastapi'. Run `pip install 'litellm[proxy]'`
2025-12-27 17:49:31,810 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2; provider = ollama
2025-12-27 17:49:31,864 - LiteLLM - ERROR - Error creating standard logging object - Missing dependency No module named 'fastapi'. Run `pip install 'litellm[proxy]'`
Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 174, in _make_common_sync_call
    response = sync_httpx_client.post(
        url=api_base,
    ...<8 lines>...
        logging_obj=logging_obj,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'http://localhost:11434/api/generate'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\main.py", line 3108, in completion
    response = base_llm_http_handler.completion(
        model=model,
    ...<13 lines>...
        client=client,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 470, in completion
    response = self._make_common_sync_call(
        sync_httpx_client=sync_httpx_client,
    ...<7 lines>...
        logging_obj=logging_obj,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 199, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 2409, in _handle_error
    raise provider_config.get_error_class(
    ...<3 lines>...
    )
litellm.llms.ollama.common_utils.OllamaError: {"error":"model 'llama3.2' not found"}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\utils.py", line 1207, in wrapper
    result = original_function(*args, **kwargs)
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\main.py", line 3452, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2270, in exception_type
    raise APIConnectionError(
    ...<4 lines>...
    )
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - {"error":"model 'llama3.2' not found"}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\proxy_server.py", line 75, in <module>
    import fastapi
ModuleNotFoundError: No module named 'fastapi'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 4370, in get_standard_logging_object_payload
    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(
        metadata=metadata,
    ...<10 lines>...
        response_id=id,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 3921, in get_standard_logging_metadata
    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(
        start_time=start_time,
        response_id=response_id,
        team_alias=clean_metadata.get("user_api_key_team_alias"),
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 4109, in _generate_cold_storage_object_key
    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\spend_tracking\cold_storage_handler.py", line 67, in _get_configured_cold_storage_custom_logger
    from litellm.proxy.proxy_server import general_settings
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\proxy_server.py", line 80, in <module>
    raise ImportError(f"Missing dependency {e}. Run `pip install 'litellm[proxy]'`")
ImportError: Missing dependency No module named 'fastapi'. Run `pip install 'litellm[proxy]'`
2025-12-27 17:49:31,874 - market_analysis_crew - ERROR - Error during stock analysis: litellm.APIConnectionError: OllamaException - {"error":"model 'llama3.2' not found"}
2025-12-27 20:01:54,526 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-12-27 20:01:56,572 - market_analysis_crew - INFO - Initializing OLLAMA LLM with model: codellama
2025-12-27 20:01:56,579 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: AAPL
2025-12-27 20:01:56,579 - market_analysis_crew - INFO - Creating specialized agents
2025-12-27 20:01:56,587 - market_analysis_crew - INFO - Creating analysis tasks for ticker: AAPL
2025-12-27 20:01:59,662 - LiteLLM - INFO - 
LiteLLM completion() model= codellama; provider = ollama
2025-12-27 20:02:08,672 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-12-27 20:02:10,726 - market_analysis_crew - INFO - Initializing OLLAMA LLM with model: codellama
2025-12-27 20:02:10,738 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: AAPL
2025-12-27 20:02:10,739 - market_analysis_crew - INFO - Creating specialized agents
2025-12-27 20:02:10,747 - market_analysis_crew - INFO - Creating analysis tasks for ticker: AAPL
2025-12-27 20:02:13,802 - LiteLLM - INFO - 
LiteLLM completion() model= codellama; provider = ollama
2025-12-27 20:02:23,719 - LiteLLM - INFO - Wrapper: Completed Call, calling success_handler
2025-12-27 20:02:23,735 - LiteLLM - ERROR - Error creating standard logging object - Missing dependency No module named 'fastapi'. Run `pip install 'litellm[proxy]'`
Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\proxy_server.py", line 75, in <module>
    import fastapi
ModuleNotFoundError: No module named 'fastapi'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 4370, in get_standard_logging_object_payload
    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(
        metadata=metadata,
    ...<10 lines>...
        response_id=id,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 3921, in get_standard_logging_metadata
    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(
        start_time=start_time,
        response_id=response_id,
        team_alias=clean_metadata.get("user_api_key_team_alias"),
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 4109, in _generate_cold_storage_object_key
    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\spend_tracking\cold_storage_handler.py", line 67, in _get_configured_cold_storage_custom_logger
    from litellm.proxy.proxy_server import general_settings
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\proxy_server.py", line 80, in <module>
    raise ImportError(f"Missing dependency {e}. Run `pip install 'litellm[proxy]'`")
ImportError: Missing dependency No module named 'fastapi'. Run `pip install 'litellm[proxy]'`
2025-12-27 20:02:23,748 - LiteLLM - ERROR - Error creating standard logging object - Missing dependency No module named 'fastapi'. Run `pip install 'litellm[proxy]'`
Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\proxy_server.py", line 75, in <module>
    import fastapi
ModuleNotFoundError: No module named 'fastapi'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 4370, in get_standard_logging_object_payload
    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(
        metadata=metadata,
    ...<10 lines>...
        response_id=id,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 3921, in get_standard_logging_metadata
    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(
        start_time=start_time,
        response_id=response_id,
        team_alias=clean_metadata.get("user_api_key_team_alias"),
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 4109, in _generate_cold_storage_object_key
    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\spend_tracking\cold_storage_handler.py", line 67, in _get_configured_cold_storage_custom_logger
    from litellm.proxy.proxy_server import general_settings
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\proxy_server.py", line 80, in <module>
    raise ImportError(f"Missing dependency {e}. Run `pip install 'litellm[proxy]'`")
ImportError: Missing dependency No module named 'fastapi'. Run `pip install 'litellm[proxy]'`
2025-12-27 20:02:26,747 - LiteLLM - INFO - 
LiteLLM completion() model= codellama; provider = ollama
2025-12-27 20:03:37,668 - market_analysis_crew - INFO - Initializing LLM...
2025-12-27 20:03:39,700 - market_analysis_crew - ERROR - Configuration error: Missing OPENAI API key. Please set OPENAI_API_KEY environment variable. Available providers with API keys: ollama
2025-12-27 20:13:16,843 - market_analysis_crew - INFO - Initializing LLM...
2025-12-27 20:13:18,859 - market_analysis_crew - ERROR - Configuration error: Missing OPENAI API key. Please set OPENAI_API_KEY environment variable. Available providers with API keys: ollama
2025-12-27 20:14:01,771 - market_analysis_crew - INFO - Initializing LLM...
2025-12-27 20:14:03,799 - market_analysis_crew - ERROR - Configuration error: Missing OPENAI API key. Please set OPENAI_API_KEY environment variable. Available providers with API keys: ollama
2025-12-27 20:14:05,866 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-12-27 20:14:07,925 - market_analysis_crew - INFO - Initializing OLLAMA LLM with model: llama3.2
2025-12-27 20:14:07,932 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: AAPL
2025-12-27 20:14:07,933 - market_analysis_crew - INFO - Creating specialized agents
2025-12-27 20:14:07,940 - market_analysis_crew - INFO - Creating analysis tasks for ticker: AAPL
2025-12-27 20:14:11,027 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2; provider = ollama
2025-12-27 20:14:11,419 - LiteLLM - ERROR - Error creating standard logging object - Missing dependency No module named 'fastapi'. Run `pip install 'litellm[proxy]'`
Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 174, in _make_common_sync_call
    response = sync_httpx_client.post(
        url=api_base,
    ...<8 lines>...
        logging_obj=logging_obj,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'http://localhost:11434/api/generate'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\main.py", line 3108, in completion
    response = base_llm_http_handler.completion(
        model=model,
    ...<13 lines>...
        client=client,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 470, in completion
    response = self._make_common_sync_call(
        sync_httpx_client=sync_httpx_client,
    ...<7 lines>...
        logging_obj=logging_obj,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 199, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 2409, in _handle_error
    raise provider_config.get_error_class(
    ...<3 lines>...
    )
litellm.llms.ollama.common_utils.OllamaError: {"error":"model 'llama3.2' not found"}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\utils.py", line 1207, in wrapper
    result = original_function(*args, **kwargs)
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\main.py", line 3452, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2270, in exception_type
    raise APIConnectionError(
    ...<4 lines>...
    )
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - {"error":"model 'llama3.2' not found"}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\proxy_server.py", line 75, in <module>
    import fastapi
ModuleNotFoundError: No module named 'fastapi'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 4370, in get_standard_logging_object_payload
    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(
        metadata=metadata,
    ...<10 lines>...
        response_id=id,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 3921, in get_standard_logging_metadata
    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(
        start_time=start_time,
        response_id=response_id,
        team_alias=clean_metadata.get("user_api_key_team_alias"),
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 4109, in _generate_cold_storage_object_key
    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\spend_tracking\cold_storage_handler.py", line 67, in _get_configured_cold_storage_custom_logger
    from litellm.proxy.proxy_server import general_settings
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\proxy_server.py", line 80, in <module>
    raise ImportError(f"Missing dependency {e}. Run `pip install 'litellm[proxy]'`")
ImportError: Missing dependency No module named 'fastapi'. Run `pip install 'litellm[proxy]'`
2025-12-27 20:14:18,444 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2; provider = ollama
2025-12-27 20:14:18,512 - LiteLLM - ERROR - Error creating standard logging object - Missing dependency No module named 'fastapi'. Run `pip install 'litellm[proxy]'`
Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 174, in _make_common_sync_call
    response = sync_httpx_client.post(
        url=api_base,
    ...<8 lines>...
        logging_obj=logging_obj,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'http://localhost:11434/api/generate'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\main.py", line 3108, in completion
    response = base_llm_http_handler.completion(
        model=model,
    ...<13 lines>...
        client=client,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 470, in completion
    response = self._make_common_sync_call(
        sync_httpx_client=sync_httpx_client,
    ...<7 lines>...
        logging_obj=logging_obj,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 199, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 2409, in _handle_error
    raise provider_config.get_error_class(
    ...<3 lines>...
    )
litellm.llms.ollama.common_utils.OllamaError: {"error":"model 'llama3.2' not found"}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\utils.py", line 1207, in wrapper
    result = original_function(*args, **kwargs)
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\main.py", line 3452, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2270, in exception_type
    raise APIConnectionError(
    ...<4 lines>...
    )
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - {"error":"model 'llama3.2' not found"}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\proxy_server.py", line 75, in <module>
    import fastapi
ModuleNotFoundError: No module named 'fastapi'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 4370, in get_standard_logging_object_payload
    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(
        metadata=metadata,
    ...<10 lines>...
        response_id=id,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 3921, in get_standard_logging_metadata
    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(
        start_time=start_time,
        response_id=response_id,
        team_alias=clean_metadata.get("user_api_key_team_alias"),
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 4109, in _generate_cold_storage_object_key
    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\spend_tracking\cold_storage_handler.py", line 67, in _get_configured_cold_storage_custom_logger
    from litellm.proxy.proxy_server import general_settings
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\proxy_server.py", line 80, in <module>
    raise ImportError(f"Missing dependency {e}. Run `pip install 'litellm[proxy]'`")
ImportError: Missing dependency No module named 'fastapi'. Run `pip install 'litellm[proxy]'`
2025-12-27 20:14:25,530 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2; provider = ollama
2025-12-27 20:14:25,569 - LiteLLM - ERROR - Error creating standard logging object - Missing dependency No module named 'fastapi'. Run `pip install 'litellm[proxy]'`
Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 174, in _make_common_sync_call
    response = sync_httpx_client.post(
        url=api_base,
    ...<8 lines>...
        logging_obj=logging_obj,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'http://localhost:11434/api/generate'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\main.py", line 3108, in completion
    response = base_llm_http_handler.completion(
        model=model,
    ...<13 lines>...
        client=client,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 470, in completion
    response = self._make_common_sync_call(
        sync_httpx_client=sync_httpx_client,
    ...<7 lines>...
        logging_obj=logging_obj,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 199, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 2409, in _handle_error
    raise provider_config.get_error_class(
    ...<3 lines>...
    )
litellm.llms.ollama.common_utils.OllamaError: {"error":"model 'llama3.2' not found"}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\utils.py", line 1207, in wrapper
    result = original_function(*args, **kwargs)
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\main.py", line 3452, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2270, in exception_type
    raise APIConnectionError(
    ...<4 lines>...
    )
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - {"error":"model 'llama3.2' not found"}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\proxy_server.py", line 75, in <module>
    import fastapi
ModuleNotFoundError: No module named 'fastapi'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 4370, in get_standard_logging_object_payload
    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(
        metadata=metadata,
    ...<10 lines>...
        response_id=id,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 3921, in get_standard_logging_metadata
    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(
        start_time=start_time,
        response_id=response_id,
        team_alias=clean_metadata.get("user_api_key_team_alias"),
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 4109, in _generate_cold_storage_object_key
    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\spend_tracking\cold_storage_handler.py", line 67, in _get_configured_cold_storage_custom_logger
    from litellm.proxy.proxy_server import general_settings
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\proxy_server.py", line 80, in <module>
    raise ImportError(f"Missing dependency {e}. Run `pip install 'litellm[proxy]'`")
ImportError: Missing dependency No module named 'fastapi'. Run `pip install 'litellm[proxy]'`
2025-12-27 20:14:25,578 - market_analysis_crew - ERROR - Error during stock analysis: litellm.APIConnectionError: OllamaException - {"error":"model 'llama3.2' not found"}
2025-12-27 20:16:51,811 - market_analysis_crew - INFO - Initializing LLM...
2025-12-27 20:16:53,821 - market_analysis_crew - ERROR - Configuration error: Missing OPENAI API key. Please set OPENAI_API_KEY environment variable. Available providers with API keys: ollama
2025-12-27 20:17:08,236 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-12-27 20:17:10,273 - market_analysis_crew - INFO - Initializing OLLAMA LLM with model: llama3.2
2025-12-27 20:17:10,284 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: AAPL
2025-12-27 20:17:10,285 - market_analysis_crew - INFO - Creating specialized agents
2025-12-27 20:17:10,296 - market_analysis_crew - INFO - Creating analysis tasks for ticker: AAPL
2025-12-27 20:17:13,389 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2; provider = ollama
2025-12-27 20:17:14,904 - LiteLLM - ERROR - Error creating standard logging object - Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`
Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 174, in _make_common_sync_call
    response = sync_httpx_client.post(
        url=api_base,
    ...<8 lines>...
        logging_obj=logging_obj,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'http://localhost:11434/api/generate'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\main.py", line 3108, in completion
    response = base_llm_http_handler.completion(
        model=model,
    ...<13 lines>...
        client=client,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 470, in completion
    response = self._make_common_sync_call(
        sync_httpx_client=sync_httpx_client,
    ...<7 lines>...
        logging_obj=logging_obj,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 199, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 2409, in _handle_error
    raise provider_config.get_error_class(
    ...<3 lines>...
    )
litellm.llms.ollama.common_utils.OllamaError: {"error":"model 'llama3.2' not found"}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\utils.py", line 1207, in wrapper
    result = original_function(*args, **kwargs)
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\main.py", line 3452, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2270, in exception_type
    raise APIConnectionError(
    ...<4 lines>...
    )
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - {"error":"model 'llama3.2' not found"}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\proxy_server.py", line 78, in <module>
    from apscheduler.schedulers.asyncio import AsyncIOScheduler
ModuleNotFoundError: No module named 'apscheduler'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 4370, in get_standard_logging_object_payload
    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(
        metadata=metadata,
    ...<10 lines>...
        response_id=id,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 3921, in get_standard_logging_metadata
    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(
        start_time=start_time,
        response_id=response_id,
        team_alias=clean_metadata.get("user_api_key_team_alias"),
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 4109, in _generate_cold_storage_object_key
    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\spend_tracking\cold_storage_handler.py", line 67, in _get_configured_cold_storage_custom_logger
    from litellm.proxy.proxy_server import general_settings
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\proxy_server.py", line 80, in <module>
    raise ImportError(f"Missing dependency {e}. Run `pip install 'litellm[proxy]'`")
ImportError: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`
2025-12-27 20:17:21,922 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2; provider = ollama
2025-12-27 20:17:21,988 - LiteLLM - ERROR - Error creating standard logging object - Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`
Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 174, in _make_common_sync_call
    response = sync_httpx_client.post(
        url=api_base,
    ...<8 lines>...
        logging_obj=logging_obj,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'http://localhost:11434/api/generate'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\main.py", line 3108, in completion
    response = base_llm_http_handler.completion(
        model=model,
    ...<13 lines>...
        client=client,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 470, in completion
    response = self._make_common_sync_call(
        sync_httpx_client=sync_httpx_client,
    ...<7 lines>...
        logging_obj=logging_obj,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 199, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 2409, in _handle_error
    raise provider_config.get_error_class(
    ...<3 lines>...
    )
litellm.llms.ollama.common_utils.OllamaError: {"error":"model 'llama3.2' not found"}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\utils.py", line 1207, in wrapper
    result = original_function(*args, **kwargs)
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\main.py", line 3452, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2270, in exception_type
    raise APIConnectionError(
    ...<4 lines>...
    )
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - {"error":"model 'llama3.2' not found"}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\proxy_server.py", line 78, in <module>
    from apscheduler.schedulers.asyncio import AsyncIOScheduler
ModuleNotFoundError: No module named 'apscheduler'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 4370, in get_standard_logging_object_payload
    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(
        metadata=metadata,
    ...<10 lines>...
        response_id=id,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 3921, in get_standard_logging_metadata
    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(
        start_time=start_time,
        response_id=response_id,
        team_alias=clean_metadata.get("user_api_key_team_alias"),
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 4109, in _generate_cold_storage_object_key
    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\spend_tracking\cold_storage_handler.py", line 67, in _get_configured_cold_storage_custom_logger
    from litellm.proxy.proxy_server import general_settings
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\proxy_server.py", line 80, in <module>
    raise ImportError(f"Missing dependency {e}. Run `pip install 'litellm[proxy]'`")
ImportError: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`
2025-12-27 20:17:29,002 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2; provider = ollama
2025-12-27 20:17:29,016 - LiteLLM - ERROR - Error creating standard logging object - Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`
Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 174, in _make_common_sync_call
    response = sync_httpx_client.post(
        url=api_base,
    ...<8 lines>...
        logging_obj=logging_obj,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'http://localhost:11434/api/generate'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\main.py", line 3108, in completion
    response = base_llm_http_handler.completion(
        model=model,
    ...<13 lines>...
        client=client,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 470, in completion
    response = self._make_common_sync_call(
        sync_httpx_client=sync_httpx_client,
    ...<7 lines>...
        logging_obj=logging_obj,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 199, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 2409, in _handle_error
    raise provider_config.get_error_class(
    ...<3 lines>...
    )
litellm.llms.ollama.common_utils.OllamaError: {"error":"model 'llama3.2' not found"}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\utils.py", line 1207, in wrapper
    result = original_function(*args, **kwargs)
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\main.py", line 3452, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2270, in exception_type
    raise APIConnectionError(
    ...<4 lines>...
    )
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - {"error":"model 'llama3.2' not found"}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\proxy_server.py", line 78, in <module>
    from apscheduler.schedulers.asyncio import AsyncIOScheduler
ModuleNotFoundError: No module named 'apscheduler'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 4370, in get_standard_logging_object_payload
    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(
        metadata=metadata,
    ...<10 lines>...
        response_id=id,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 3921, in get_standard_logging_metadata
    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(
        start_time=start_time,
        response_id=response_id,
        team_alias=clean_metadata.get("user_api_key_team_alias"),
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 4109, in _generate_cold_storage_object_key
    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\spend_tracking\cold_storage_handler.py", line 67, in _get_configured_cold_storage_custom_logger
    from litellm.proxy.proxy_server import general_settings
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\proxy_server.py", line 80, in <module>
    raise ImportError(f"Missing dependency {e}. Run `pip install 'litellm[proxy]'`")
ImportError: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`
2025-12-27 20:17:29,026 - market_analysis_crew - ERROR - Error during stock analysis: litellm.APIConnectionError: OllamaException - {"error":"model 'llama3.2' not found"}
2025-12-27 20:21:45,733 - market_analysis_crew - INFO - Initializing LLM...
2025-12-27 20:21:47,765 - market_analysis_crew - ERROR - Configuration error: Missing OPENAI API key. Please set OPENAI_API_KEY environment variable. Available providers with API keys: dashscope, ollama
2025-12-27 20:22:22,724 - market_analysis_crew - INFO - Initializing LLM...
2025-12-27 20:22:24,732 - market_analysis_crew - ERROR - Configuration error: Missing OPENAI API key. Please set OPENAI_API_KEY environment variable. Available providers with API keys: dashscope, ollama
2025-12-27 20:22:41,349 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-12-27 20:22:41,350 - market_analysis_crew - INFO - Initializing DASHSCOPE LLM with model: qwen3-coder-plus
2025-12-27 20:22:41,357 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: AAPL
2025-12-27 20:22:41,358 - market_analysis_crew - INFO - Creating specialized agents
2025-12-27 20:22:41,366 - market_analysis_crew - INFO - Creating analysis tasks for ticker: AAPL
2025-12-27 20:22:44,439 - LiteLLM - INFO - 
LiteLLM completion() model= qwen3-coder-plus; provider = dashscope
2025-12-27 20:22:48,403 - LiteLLM - ERROR - Error creating standard logging object - Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`
Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 736, in completion
    raise e
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 664, in completion
    ) = self.make_sync_openai_chat_completion_request(
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        openai_client=openai_client,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
        logging_obj=logging_obj,
        ^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\logging_utils.py", line 149, in sync_wrapper
    result = func(*args, **kwargs)
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 482, in make_sync_openai_chat_completion_request
    raise e
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 464, in make_sync_openai_chat_completion_request
    raw_response = openai_client.chat.completions.with_raw_response.create(
        **data, timeout=timeout
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\openai\_legacy_response.py", line 364, in wrapped
    return cast(LegacyAPIResponse[R], func(*args, **kwargs))
                                      ~~~~^^^^^^^^^^^^^^^^^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\openai\_utils\_utils.py", line 287, in wrapper
    return func(*args, **kwargs)
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 925, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<43 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\openai\_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\openai\_base_client.py", line 1037, in request
    raise self._make_status_error_from_response(err.response) from None
openai.AuthenticationError: Error code: 401 - {'error': {'message': 'Incorrect API key provided. For details, see: https://www.alibabacloud.com/help/en/model-studio/error-code#apikey-error', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}, 'request_id': 'c7bf446d-fcd0-938b-972b-d7e660b10d96'}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\main.py", line 1973, in completion
    raise e
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\main.py", line 1946, in completion
    response = openai_chat_completions.completion(
        model=model,
    ...<15 lines>...
        custom_llm_provider=custom_llm_provider,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\openai\openai.py", line 747, in completion
    raise OpenAIError(
    ...<4 lines>...
    )
litellm.llms.openai.common_utils.OpenAIError: Error code: 401 - {'error': {'message': 'Incorrect API key provided. For details, see: https://www.alibabacloud.com/help/en/model-studio/error-code#apikey-error', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}, 'request_id': 'c7bf446d-fcd0-938b-972b-d7e660b10d96'}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\utils.py", line 1207, in wrapper
    result = original_function(*args, **kwargs)
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\main.py", line 3452, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 456, in exception_type
    raise AuthenticationError(
    ...<5 lines>...
    )
litellm.exceptions.AuthenticationError: litellm.AuthenticationError: AuthenticationError: DashscopeException - Incorrect API key provided. For details, see: https://www.alibabacloud.com/help/en/model-studio/error-code#apikey-error

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\proxy_server.py", line 78, in <module>
    from apscheduler.schedulers.asyncio import AsyncIOScheduler
ModuleNotFoundError: No module named 'apscheduler'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 4370, in get_standard_logging_object_payload
    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(
        metadata=metadata,
    ...<10 lines>...
        response_id=id,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 3921, in get_standard_logging_metadata
    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(
        start_time=start_time,
        response_id=response_id,
        team_alias=clean_metadata.get("user_api_key_team_alias"),
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 4109, in _generate_cold_storage_object_key
    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\spend_tracking\cold_storage_handler.py", line 67, in _get_configured_cold_storage_custom_logger
    from litellm.proxy.proxy_server import general_settings
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\proxy_server.py", line 80, in <module>
    raise ImportError(f"Missing dependency {e}. Run `pip install 'litellm[proxy]'`")
ImportError: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`
2025-12-27 20:22:48,417 - market_analysis_crew - ERROR - Error during stock analysis: litellm.AuthenticationError: AuthenticationError: DashscopeException - Incorrect API key provided. For details, see: https://www.alibabacloud.com/help/en/model-studio/error-code#apikey-error
2025-12-27 20:23:38,238 - market_analysis_crew - INFO - Initializing LLM...
2025-12-27 20:23:40,260 - market_analysis_crew - ERROR - Configuration error: Missing DASHSCOPE API key. Please set DASHSCOPE_API_KEY environment variable. Available providers with API keys: ollama
2025-12-27 20:23:47,963 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-12-27 20:23:50,014 - market_analysis_crew - INFO - Initializing OLLAMA LLM with model: llama3.2
2025-12-27 20:23:50,018 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: AAPL
2025-12-27 20:23:50,019 - market_analysis_crew - INFO - Creating specialized agents
2025-12-27 20:23:50,023 - market_analysis_crew - INFO - Creating analysis tasks for ticker: AAPL
2025-12-27 20:23:53,069 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2; provider = ollama
2025-12-27 20:23:53,664 - LiteLLM - ERROR - Error creating standard logging object - Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`
Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 174, in _make_common_sync_call
    response = sync_httpx_client.post(
        url=api_base,
    ...<8 lines>...
        logging_obj=logging_obj,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'http://localhost:11434/api/generate'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\main.py", line 3108, in completion
    response = base_llm_http_handler.completion(
        model=model,
    ...<13 lines>...
        client=client,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 470, in completion
    response = self._make_common_sync_call(
        sync_httpx_client=sync_httpx_client,
    ...<7 lines>...
        logging_obj=logging_obj,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 199, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 2409, in _handle_error
    raise provider_config.get_error_class(
    ...<3 lines>...
    )
litellm.llms.ollama.common_utils.OllamaError: {"error":"model 'llama3.2' not found"}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\utils.py", line 1207, in wrapper
    result = original_function(*args, **kwargs)
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\main.py", line 3452, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2270, in exception_type
    raise APIConnectionError(
    ...<4 lines>...
    )
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - {"error":"model 'llama3.2' not found"}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\proxy_server.py", line 78, in <module>
    from apscheduler.schedulers.asyncio import AsyncIOScheduler
ModuleNotFoundError: No module named 'apscheduler'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 4370, in get_standard_logging_object_payload
    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(
        metadata=metadata,
    ...<10 lines>...
        response_id=id,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 3921, in get_standard_logging_metadata
    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(
        start_time=start_time,
        response_id=response_id,
        team_alias=clean_metadata.get("user_api_key_team_alias"),
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 4109, in _generate_cold_storage_object_key
    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\spend_tracking\cold_storage_handler.py", line 67, in _get_configured_cold_storage_custom_logger
    from litellm.proxy.proxy_server import general_settings
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\proxy_server.py", line 80, in <module>
    raise ImportError(f"Missing dependency {e}. Run `pip install 'litellm[proxy]'`")
ImportError: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`
2025-12-27 20:24:35,864 - market_analysis_crew - INFO - Initializing LLM...
2025-12-27 20:24:37,891 - market_analysis_crew - ERROR - Configuration error: Missing DASHSCOPE API key. Please set DASHSCOPE_API_KEY environment variable. Available providers with API keys: ollama
2025-12-27 20:24:44,712 - market_analysis_crew - INFO - Initializing Market Analysis Crew
2025-12-27 20:24:46,753 - market_analysis_crew - INFO - Initializing OLLAMA LLM with model: llama3.2
2025-12-27 20:24:46,756 - market_analysis_crew - INFO - Starting comprehensive analysis for ticker: AAPL
2025-12-27 20:24:46,756 - market_analysis_crew - INFO - Creating specialized agents
2025-12-27 20:24:46,759 - market_analysis_crew - INFO - Creating analysis tasks for ticker: AAPL
2025-12-27 20:24:49,821 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2; provider = ollama
2025-12-27 20:24:50,412 - LiteLLM - ERROR - Error creating standard logging object - Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`
Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 174, in _make_common_sync_call
    response = sync_httpx_client.post(
        url=api_base,
    ...<8 lines>...
        logging_obj=logging_obj,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'http://localhost:11434/api/generate'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\main.py", line 3108, in completion
    response = base_llm_http_handler.completion(
        model=model,
    ...<13 lines>...
        client=client,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 470, in completion
    response = self._make_common_sync_call(
        sync_httpx_client=sync_httpx_client,
    ...<7 lines>...
        logging_obj=logging_obj,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 199, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 2409, in _handle_error
    raise provider_config.get_error_class(
    ...<3 lines>...
    )
litellm.llms.ollama.common_utils.OllamaError: {"error":"model 'llama3.2' not found"}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\utils.py", line 1207, in wrapper
    result = original_function(*args, **kwargs)
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\main.py", line 3452, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2270, in exception_type
    raise APIConnectionError(
    ...<4 lines>...
    )
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - {"error":"model 'llama3.2' not found"}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\proxy_server.py", line 78, in <module>
    from apscheduler.schedulers.asyncio import AsyncIOScheduler
ModuleNotFoundError: No module named 'apscheduler'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 4370, in get_standard_logging_object_payload
    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(
        metadata=metadata,
    ...<10 lines>...
        response_id=id,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 3921, in get_standard_logging_metadata
    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(
        start_time=start_time,
        response_id=response_id,
        team_alias=clean_metadata.get("user_api_key_team_alias"),
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 4109, in _generate_cold_storage_object_key
    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\spend_tracking\cold_storage_handler.py", line 67, in _get_configured_cold_storage_custom_logger
    from litellm.proxy.proxy_server import general_settings
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\proxy_server.py", line 80, in <module>
    raise ImportError(f"Missing dependency {e}. Run `pip install 'litellm[proxy]'`")
ImportError: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`
2025-12-27 20:24:57,435 - LiteLLM - INFO - 
LiteLLM completion() model= llama3.2; provider = ollama
2025-12-27 20:24:57,519 - LiteLLM - ERROR - Error creating standard logging object - Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`
Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 174, in _make_common_sync_call
    response = sync_httpx_client.post(
        url=api_base,
    ...<8 lines>...
        logging_obj=logging_obj,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'http://localhost:11434/api/generate'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\main.py", line 3108, in completion
    response = base_llm_http_handler.completion(
        model=model,
    ...<13 lines>...
        client=client,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 470, in completion
    response = self._make_common_sync_call(
        sync_httpx_client=sync_httpx_client,
    ...<7 lines>...
        logging_obj=logging_obj,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 199, in _make_common_sync_call
    raise self._handle_error(e=e, provider_config=provider_config)
          ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\llms\custom_httpx\llm_http_handler.py", line 2409, in _handle_error
    raise provider_config.get_error_class(
    ...<3 lines>...
    )
litellm.llms.ollama.common_utils.OllamaError: {"error":"model 'llama3.2' not found"}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\utils.py", line 1207, in wrapper
    result = original_function(*args, **kwargs)
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\main.py", line 3452, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2270, in exception_type
    raise APIConnectionError(
    ...<4 lines>...
    )
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: OllamaException - {"error":"model 'llama3.2' not found"}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\proxy_server.py", line 78, in <module>
    from apscheduler.schedulers.asyncio import AsyncIOScheduler
ModuleNotFoundError: No module named 'apscheduler'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 4370, in get_standard_logging_object_payload
    clean_metadata = StandardLoggingPayloadSetup.get_standard_logging_metadata(
        metadata=metadata,
    ...<10 lines>...
        response_id=id,
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 3921, in get_standard_logging_metadata
    cold_storage_object_key = StandardLoggingPayloadSetup._generate_cold_storage_object_key(
        start_time=start_time,
        response_id=response_id,
        team_alias=clean_metadata.get("user_api_key_team_alias"),
    )
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\litellm_core_utils\litellm_logging.py", line 4109, in _generate_cold_storage_object_key
    configured_cold_storage_logger = ColdStorageHandler._get_configured_cold_storage_custom_logger()
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\spend_tracking\cold_storage_handler.py", line 67, in _get_configured_cold_storage_custom_logger
    from litellm.proxy.proxy_server import general_settings
  File "D:\dev\Agentic-AI-Stock-Analysis-Crew.git\.venv\Lib\site-packages\litellm\proxy\proxy_server.py", line 80, in <module>
    raise ImportError(f"Missing dependency {e}. Run `pip install 'litellm[proxy]'`")
ImportError: Missing dependency No module named 'apscheduler'. Run `pip install 'litellm[proxy]'`
2025-12-27 20:25:04,549 - market_analysis_crew - ERROR - Error during stock analysis: cannot schedule new futures after shutdown
